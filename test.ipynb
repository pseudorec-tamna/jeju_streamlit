{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "\n",
    "template_chat = '''ë‹¹ì‹ ì€ íƒë¼ëŠ” ë§›ì˜ íƒë‚˜ ëª¨ë¸ì…ë‹ˆë‹¤. \n",
    "ì‚¬ìš©ìê°€ ë‹¹ì‹ ì—ê²Œ ëˆ„êµ°ì§€ ë¬¼ìœ¼ë©´ 'ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ëŠ” íƒë‚˜ë¼ê³  ì†Œê°œí•˜ì‹­ì‹œì˜¤. \n",
    "ê¸ì •ì ì´ê³  ë°œë„í•˜ê²Œ ì œì£¼ë„ë¯¼ì˜ ëŠë‚Œì„ ì‚´ë ¤ì„œ ì§ˆë¬¸ì˜ ë‹µë³€ì„ ë„ì™€ì£¼ì„¸ìš”.\n",
    "ì°¸ê³ ë¡œ ëª¨ë“  ë‹µë³€ì€ ëª¨ë‘ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”. \n",
    "\n",
    "ë‹¹ì‹ ì´ í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. \n",
    "- ê·¼ì²˜ ë§›ì§‘ ì¶”ì²œ : ì‚¬ìš©ìì˜ í˜„ì¬ ìœ„ì¹˜ í˜¹ì€ ì›í•˜ëŠ” ì¥ì†Œì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ë§›ì§‘ì„ ì¶”ì²œí•´ì¤ë‹ˆë‹¤.(ì£¼ì†Œë¥¼ ìµœëŒ€í•œ ìì„¸í•˜ê²Œ ì•Œë ¤ì£¼ì„¸ìš”.) ì˜ˆ) ì œì£¼ì‹œ ì• ì›”ì ê°€ë¬¸ë™ê¸¸ 27-8 ì œì£¼ë‹¬ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”. \n",
    "- ë‹¤ìŒì— ê°ˆ ì¥ì†Œ ì¶”ì²œ : ì‚¬ìš©ìê°€ ë§ˆì§€ë§‰ì— ë“¤ë¦° ì¥ì†Œë¡œë¶€í„° ë‹¤ìŒìœ¼ë¡œ ê°€ì¥ ë§ì´ ë°©ë¬¸í•˜ëŠ” ë§›ì§‘, ì¹´í˜, ìˆ ì§‘, ê´€ê´‘ì§€ë“±ì„ ì¶”ì²œí•´ì¤ë‹ˆë‹¤.\n",
    "- ì†ì„±ì— ê¸°ë°˜í•œ ì¶”ì²œ : ì—…ì¢…, í‰ê· ì´ìš©ê¸ˆì•¡, í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘ ë“±ì„ ìš”ì²­í•´ì£¼ì‹œë©´ ì´ë¥¼ ê³ ë ¤í•´ì„œ ë§›ì§‘ì„ ì¶”ì²œí•´ì¤ë‹ˆë‹¤. \n",
    "'''\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template_chat),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "recommendation_template_chat = '''ë‹¹ì‹ ì€ íƒë¼ëŠ” ë§›ì˜ íƒë‚˜ ëª¨ë¸ì…ë‹ˆë‹¤. \n",
    "ì‚¬ìš©ìê°€ ë‹¹ì‹ ì—ê²Œ ëˆ„êµ°ì§€ ë¬¼ìœ¼ë©´ 'ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ëŠ” íƒë‚˜ë¼ê³  ì†Œê°œí•˜ì‹­ì‹œì˜¤. \n",
    "ì•„ë˜ì˜ ì£¼ì–´ì§„ <ì¶”ì²œ ê²°ê³¼> ë° <ê±°ë¦¬ ì •ë³´>ë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì˜ ë‹µë³€ì„ ë„ì™€ì£¼ì„¸ìš”. \n",
    "ì°¸ê³ ë¡œ ëª¨ë“  ë‹µë³€ì€ ëª¨ë‘ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”. \n",
    "\n",
    "<ì¶”ì²œ ê²°ê³¼> \n",
    "{recommendations}\n",
    "\n",
    "<ë‹µë³€ í¬ë§·> \n",
    "ğŸ¬ ê°€ê²Œëª…: ã…‡ã…‡ã…‡\n",
    "ğŸ¥ ì—…ì¢…: ã…‡ã…‡ã…‡\n",
    "ğŸ“„ ëŒ€í‘œ ë©”ë‰´: ã…‡ã…‡ã…‡\n",
    "ğŸ•´ï¸ ì£¼ì†Œ: ã…‡ã…‡ã…‡\n",
    "ğŸ“„ ì˜ì—…ì‹œê°„: ã…‡ã…‡ã…‡\n",
    "ğŸ“„ ì˜ˆì•½ ìœ ë¬´: ã…‡ã…‡ã…‡\n",
    "ğŸ“„ ì£¼ì°¨ ìœ ë¬´: ã…‡ã…‡ã…‡\n",
    "ğŸ“„ ì¶”ì²œ ì´ìœ : ã…‡ã…‡ã…‡\n",
    "'''\n",
    "\n",
    "recommendation_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", recommendation_template_chat),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "recommendation_sql_template_chat = '''ë‹¹ì‹ ì€ íƒë¼ëŠ” ë§›ì˜ íƒë‚˜ ëª¨ë¸ì…ë‹ˆë‹¤. \n",
    "ì‚¬ìš©ìê°€ ë‹¹ì‹ ì—ê²Œ ëˆ„êµ°ì§€ ë¬¼ìœ¼ë©´ 'ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ëŠ” íƒë‚˜ë¼ê³  ì†Œê°œí•˜ì‹­ì‹œì˜¤. \n",
    "ì•„ë˜ì˜ ì£¼ì–´ì§„ <ì¶”ì²œ ê²°ê³¼> ë° <ê²€ìƒ‰ ì •ë³´>ë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì˜ ë‹µë³€ì„ ë„ì™€ì£¼ì„¸ìš”. \n",
    "ì°¸ê³ ë¡œ ëª¨ë“  ë‹µë³€ì€ ëª¨ë‘ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”. \n",
    "\n",
    "<ê²€ìƒ‰ ì •ë³´>\n",
    "{search_info}\n",
    "\n",
    "<ì¶”ì²œ ê²°ê³¼> \n",
    "{recommendations}\n",
    "\n",
    "ë‹µë³€ì˜ í¬ë§·ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. \n",
    "ğŸ¬ ê°€ê²Œëª…: ã…‡ã…‡ã…‡\n",
    "ğŸ¥ ì—…ì¢…: ã…‡ã…‡ã…‡\n",
    "ğŸ“„ ëŒ€í‘œ ë©”ë‰´: ã…‡ã…‡ã…‡\n",
    "ğŸ•´ï¸ ì£¼ì†Œ: ã…‡ã…‡ã…‡\n",
    "ğŸ“„ ì˜ì—…ì‹œê°„: ã…‡ã…‡ã…‡\n",
    "ğŸ“„ ì˜ˆì•½ ìœ ë¬´: ã…‡ã…‡ã…‡\n",
    "ğŸ“„ ì£¼ì°¨ ìœ ë¬´: ã…‡ã…‡ã…‡\n",
    "ğŸ“„ ì¶”ì²œ ì´ìœ : ã…‡ã…‡ã…‡\n",
    "'''\n",
    "\n",
    "recommendation_sql_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", recommendation_sql_template_chat),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "item_serach_template_chat = '''ë‹¹ì‹ ì€ íƒë¼ëŠ” ë§›ì˜ íƒë‚˜ ëª¨ë¸ì…ë‹ˆë‹¤. \n",
    "ì‚¬ìš©ìê°€ ë‹¹ì‹ ì—ê²Œ ëˆ„êµ°ì§€ ë¬¼ìœ¼ë©´ 'ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ëŠ” íƒë‚˜ë¼ê³  ì†Œê°œí•˜ì‹­ì‹œì˜¤. \n",
    "ì•„ë˜ì˜ ì£¼ì–´ì§„ <ê°€ê²Œ ì •ë³´>ë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì˜ ë‹µë³€ì„ ë„ì™€ì£¼ì„¸ìš”.\n",
    "ì°¸ê³ ë¡œ ëª¨ë“  ë‹µë³€ì€ ëª¨ë‘ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”. \n",
    "\n",
    "<ê°€ê²Œ ì •ë³´> \n",
    "- ê°€ê²Œëª… : {MCT_NM}\n",
    "- ìœ„ì¹˜ : {ADDR}\n",
    "- ì „í™”ë²ˆí˜¸ : {tel}\n",
    "- ì˜ˆì•½ ê°€ëŠ¥ ìœ ë¬´ : {booking}\n",
    "'''\n",
    "\n",
    "item_search_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", item_serach_template_chat),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from utils.chat_state import ChatState\n",
    "from utils.prompts import chat_prompt_template, recommendation_prompt_template, recommendation_sql_prompt_template, item_search_prompt_template\n",
    "\n",
    "from recommendation.prompt import sub_task_detection_prompt\n",
    "from recommendation.utils import json_format\n",
    "# from colorama import Fore, Style\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from recommendation.utils import sub_task_detection\n",
    "from recommendation.distance_based import distance_based_recommendation, get_coordinates_by_question, coordinates_based_recommendation\n",
    "import requests, time\n",
    "import subprocess\n",
    "from recommendation.sql_based import extract_sql_query, sql_based_recommendation\n",
    "from recommendation.prompt import template_sql_prompt\n",
    "# from tamla import load_memory\n",
    "from utils.lang_utils import pairwise_chat_history_to_msg_list\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./data/additional_info.csv\", encoding='cp949')\n",
    "df = df.drop_duplicates(subset=[\"MCT_NM\"], keep=\"last\")\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "database = pd.read_csv(\"./data/JEJU_MCT_DATA_v2.csv\", encoding='cp949')\n",
    "meta_info = database.drop_duplicates(subset=[\"MCT_NM\"], keep=\"last\")\n",
    "df = df.merge(meta_info[[\"MCT_NM\", \"MCT_TYPE\"]], how=\"left\", on=\"MCT_NM\")\n",
    "\n",
    "def load_memory(input, chat_state):\n",
    "    # print(\"chat_state:\", chat_state.memory)\n",
    "    memory_vars = chat_state.memory.load_memory_variables({})\n",
    "    memory_vars[\"chat_history\"] = pairwise_chat_history_to_msg_list(chat_state.chat_history)\n",
    "    # print(\"chat_history:\", memory_vars[\"chat_history\"])\n",
    "    # memory_vars.get(\"chat_history\", [])\n",
    "    return memory_vars.get(\"chat_history\", [])\n",
    "\n",
    "def get_hw_response(chat_state: ChatState):\n",
    "    # Initialize the Gemini 1.5 Flash LLM\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=chat_state.bot_settings.llm_model_name,\n",
    "        google_api_key=chat_state.google_api_key\n",
    "    )\n",
    "\n",
    "    response = sub_task_detection(chat_state.message)\n",
    "    response_type = json_format(response)[\"response_type\"]\n",
    "\n",
    "    if response_type == \"Chat\":\n",
    "        chain = RunnablePassthrough.assign(chat_history=lambda input: load_memory(input, chat_state)) | chat_prompt_template | llm\n",
    "        result = chain.invoke({\"question\": chat_state.message})\n",
    "\n",
    "    elif response_type == \"Recommendation\":\n",
    "        \n",
    "        if json_format(response)[\"recommendation_type\"] == \"Distance-based\":\n",
    "            chain = RunnablePassthrough.assign(chat_history=lambda input: load_memory(input, chat_state)) | recommendation_prompt_template | llm\n",
    "            coord = get_coordinates_by_question(chat_state.message)\n",
    "            print(\"ì •í™•í•œ ì£¼ì†Œë¥¼ ì§€ë„ì—ì„œ ê²€ìƒ‰í›„ì— í´ë¦­í•´ì£¼ì„¸ìš” !!\")\n",
    "\n",
    "            from IPython.display import IFrame\n",
    "            latitude, longitude = coord  # coordì—ì„œ ìœ„ë„ì™€ ê²½ë„ ì¶”ì¶œ\n",
    "            display(IFrame(src='http://127.0.0.1:5000', width=1200, height=600))\n",
    "            \n",
    "            while True: \n",
    "                response = requests.get('http://127.0.0.1:5000/get_coordinates')\n",
    "                coordinates = response.json()\n",
    "                latitude = coordinates['latitude']\n",
    "                longitude = coordinates['longitude']\n",
    "                if latitude is not None and longitude is not None:\n",
    "                    break\n",
    "                time.sleep(5)\n",
    "            \n",
    "            rec = coordinates_based_recommendation((longitude, latitude), df)\n",
    "            result = chain.invoke({\"question\": chat_state.message, \"recommendations\": rec})  \n",
    "        elif json_format(response)[\"recommendation_type\"] == \"Attribute-based\":\n",
    "            chain = RunnablePassthrough.assign(chat_history=lambda input: load_memory(input, chat_state)) | recommendation_sql_prompt_template | llm\n",
    "            sql_prompt = ChatPromptTemplate.from_template(template_sql_prompt)\n",
    "            sql_chain = sql_prompt | llm\n",
    "            output = sql_chain.invoke({\"question\": chat_state.message})            \n",
    "            rec = sql_based_recommendation(output, df)\n",
    "            result = chain.invoke({\"question\": chat_state.message, \"recommendations\": rec, \"search_info\": output.content})  \n",
    "        else: \n",
    "            pass \n",
    "              \n",
    "    elif response_type == \"Item Detail Search\":\n",
    "        # SQL ë¬¸ìœ¼ë¡œ ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡ \n",
    "\n",
    "        chain = RunnablePassthrough.assign(chat_history=lambda input: load_memory(input, chat_state)) | item_search_prompt_template | llm\n",
    "        item_info = df.loc[df[\"MCT_NM\"] == str(chat_state.message)].reset_index(drop=True)\n",
    "        print(\"item_info:\", item_info)\n",
    "        result = chain.invoke({\n",
    "            \"question\": chat_state.message,\n",
    "            \"MCT_NM\": item_info[\"MCT_NM\"],\n",
    "            \"ADDR\": item_info[\"ADDR\"],\n",
    "            \"tel\": item_info[\"tel\"],\n",
    "            \"booking\": item_info[\"booking\"]\n",
    "            })\n",
    "    else: \n",
    "        pass \n",
    "    response = {\"answer\": result.content}\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gyungah/Jupyter/Pseudo/Jeju_chatbot-main/utils/type_utils.py:145: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
      "/Users/gyungah/Jupyter/Pseudo/Jeju_chatbot-main/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from agents.greeting_quick import jeju_weather_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jeju_weather_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
